package main.java.crossValidation;

import io.jenetics.Mutator;
import io.jenetics.RouletteWheelSelector;
import io.jenetics.engine.Engine;
import io.jenetics.engine.EvolutionResult;
import io.jenetics.engine.Limits;
import io.jenetics.ext.SingleNodeCrossover;
import io.jenetics.ext.util.TreeNode;
import io.jenetics.prog.ProgramGene;
import io.jenetics.prog.op.MathExpr;
import io.jenetics.prog.op.Op;
import main.java.classificationProblem.Classification;
import main.java.classificationProblem.FitnessFunction;
import main.java.classificationProblem.GenerateProblem;
import main.java.datasetAccess.Dataset;
import main.java.parameterSettings.ParameterSettings;
import main.java.runDataTextFileWriter.RunDataTextFileWriter;

/**
 * Class implements cross-validation systems.
 * @author Joshua McDonagh
 *
 */
public class CrossValidation 
{
	private FoldData foldData;
	
	private int k = 5;
	
	/**
	 * Constructor method.
	 * @param dataset	Dataset to be utilised
	 */
	public CrossValidation(Dataset dataset)
	{
		foldData = new FoldData(k, dataset);
	}
	
	/**
	 * Runs cross-validation on the GP system using given parameter settings.
	 * @param parameters	Parameter settings for GP training
	 * @return FitnessInfo instance containing the fitness information from the cross-validation process
	 * @throws InterruptedException
	 */
	public FitnessInfo run(ParameterSettings parameters) throws InterruptedException
	{
		FitnessInfo fitnessInfo = new FitnessInfo();
		RunDataTextFileWriter writer = RunDataTextFileWriter.get();
		
		System.out.println("Cross-Validation started...");
		writer.write("\n---- " + k + "-Fold Cross-Validation Process ----");
		for (int i = 0; i < k; i++)
		{
			System.out.println("Starting fold " + (i + 1) + " of " + k + "...");
			writer.write("\nFold " + (i + 1) + " of " + k);
			final TreeNode<Op<Double>> tree = TrainGP(parameters, foldData.getTrainingSet(i));
			fitnessInfo = TestTree(tree, parameters, foldData.getTestingSet(i), fitnessInfo);
		}
		System.out.println("Cross-Validation ended...");
		writer.write("\n---- Final Results ----");
		
		double accuracyAverage = fitnessInfo.getAccuracyAverage();
		double precisionAverage = fitnessInfo.getPrecisionAverage();
		double recallAverage = fitnessInfo.getRecallAverage();
		double fMeasureAverage = fitnessInfo.getFMeasureAverage();
		
		System.out.println("Average accuracy: " + accuracyAverage);
		System.out.println("Average precision: " + precisionAverage);
		System.out.println("Average recall: " + recallAverage);
		System.out.println("Average f-measure: " + fMeasureAverage);
		
		writer.write("Average accuracy: " + accuracyAverage);
		writer.write("Average precision: " + precisionAverage);
		writer.write("Average recall: " + recallAverage);
		writer.write("Average f-measure: " + fMeasureAverage);
		
		return fitnessInfo;
	}
	
	/**
	 * Trains the GP system on the given parameters and dataset.
	 * @param parameters	Parameter settings for GP training
	 * @param dataset		Dataset for GP training on
	 * @return tree program generated by the GP training process
	 * @throws InterruptedException
	 */
	private TreeNode<Op<Double>> TrainGP(ParameterSettings parameters, Dataset dataset) throws InterruptedException
	{	
		RunDataTextFileWriter writer = RunDataTextFileWriter.get();
		
		System.out.println("Training...");
		writer.write("Training: ");
		
		final Classification<Double> CLASSIFICATION = GenerateProblem.classification(parameters, parameters.fitnessFunction(), dataset);
		
		final Engine<ProgramGene<Double>, Double> engine  = Engine.builder(CLASSIFICATION).maximizing()
				.survivorsSelector(new RouletteWheelSelector<>())
				.offspringSelector(new RouletteWheelSelector<>())
				.alterers(
						new SingleNodeCrossover<>(parameters.crossoverProbability()),
						new Mutator<>(parameters.mutationProbability()))
				.build();
		
		final EvolutionResult<ProgramGene<Double>, Double> result = engine
				.stream()
				.limit(Limits.byFitnessThreshold(1.0))
				.limit(parameters.maxGeneration())
				.collect(EvolutionResult.toBestEvolutionResult());
		
		final ProgramGene<Double> program = result.bestPhenotype()
				.genotype()
				.gene();
		
		final TreeNode<Op<Double>> tree = program.toTreeNode();
	    MathExpr.rewrite(tree);
	    double fitness = CLASSIFICATION.fitnessEval(tree);
	    System.out.println("Generations: " + result.totalGenerations());
	    System.out.println("Depth: " + tree.depth());
	    System.out.println(parameters.fitnessFunctionName() + " fitness: " + fitness);
	    
	    writer.write("	Generations: " + result.totalGenerations());
	    writer.write("	Function: " + new MathExpr(tree));
	    writer.write("	Depth: " + tree.depth());
	    writer.write("	" + parameters.fitnessFunctionName() + " Fitness: " + fitness);

	    return tree;
	}
	
	/**
	 * Runs the given tree using the dataset given as the test set.
	 * @param tree			Tree to run the test set on
	 * @param parameters	Parameter settings for GP training
	 * @param dataset		Testing dataset
	 * @param fitnessInfo	FitnessInfo instance which holds fitness data for the GP run
	 * @return FitnessInfo instance with updated fitness data for the current tree test
	 */
	private FitnessInfo TestTree(TreeNode<Op<Double>> tree, ParameterSettings parameters, Dataset dataset, FitnessInfo fitnessInfo)
	{
		RunDataTextFileWriter writer = RunDataTextFileWriter.get();
		
		System.out.println("Testing...");
		writer.write("Testing Fitness: ");
		
		// Calculate accuracy
		Classification<Double> CLASSIFICATION = GenerateProblem.classification(parameters, FitnessFunction::accuracy, dataset);
		double fitness = CLASSIFICATION.fitnessEval(tree);
		fitnessInfo.enterAccuracyVal(fitness);
		
		System.out.println("Accuracy: " + fitness);
		writer.write("	Accuracy: " + fitness);
		
		// Calculate precision
		CLASSIFICATION = GenerateProblem.classification(parameters, FitnessFunction::precision, dataset);
		fitness = CLASSIFICATION.fitnessEval(tree);
		fitnessInfo.enterPrecisionVal(fitness);
		
		System.out.println("Precision: " + fitness);
		writer.write("	Precision: " + fitness);
		
		// Calculate recall
		CLASSIFICATION = GenerateProblem.classification(parameters, FitnessFunction::recall, dataset);
		fitness = CLASSIFICATION.fitnessEval(tree);
		fitnessInfo.enterRecallVal(fitness);
		
		System.out.println("Recall: " + fitness);
		writer.write("	Recall: " + fitness);
		
		// Calculate F-measure
		CLASSIFICATION = GenerateProblem.classification(parameters, FitnessFunction::fMeasure, dataset);
		fitness = CLASSIFICATION.fitnessEval(tree);
		fitnessInfo.enterFMeasureVal(fitness);
		
		System.out.println("F-measure: " + fitness);
		writer.write("	F-Measure: " + fitness);
		
		return fitnessInfo;
	}
}
